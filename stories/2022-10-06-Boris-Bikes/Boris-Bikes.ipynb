{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dacb20",
   "metadata": {},
   "source": [
    "# Boris Bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9447d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import h3 # uber geo package\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import folium\n",
    "\n",
    "import requests as requests\n",
    "from geopandas import GeoDataFrame, points_from_xy\n",
    "import movingpandas as mpd\n",
    "from shapely.geometry import Point\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "\n",
    "import re\n",
    "import contextily as cx\n",
    "import community\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pyproj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir()\n",
    "\n",
    "!echo $PROJ_LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768144d",
   "metadata": {},
   "source": [
    "TODO: this needs doesn't persist in initial notebook run and has to be run again separately and charts redrawn :( - fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44103ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd764e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be735b0",
   "metadata": {},
   "source": [
    "# Data description and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751f8ed",
   "metadata": {},
   "source": [
    "Explain the steps for the data description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f260d3",
   "metadata": {},
   "source": [
    "## Load processed data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba177559",
   "metadata": {},
   "source": [
    "load reference data for bike station locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update\n",
    "RAW = \"../data/BorisBikes_journeys_cleaned_data.pickle\"\n",
    "\n",
    "LOCATION_REF = \"../data/BorisBikes_stations_coordinates.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3583ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_locations_df = pd.read_json(LOCATION_REF).T\n",
    "\n",
    "station_locations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa5508",
   "metadata": {},
   "source": [
    "main data, load with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "df = pd.read_pickle(RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bade7d0",
   "metadata": {},
   "source": [
    "about 100M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268ac79",
   "metadata": {},
   "source": [
    "some cleaning of bad dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLIEST_DATE = datetime.datetime(2010, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# filter out of range dates\n",
    "df = df[df[\"start_date\"] > EARLIEST_DATE]\n",
    "# allow NA for end dates\n",
    "df = df[(df[\"end_date\"] > EARLIEST_DATE) | df[\"end_date\"].isna()]\n",
    "\n",
    "# also drop entries where start date before end date\n",
    "df = df[df[\"start_date\"] < df[\"end_date\"]]\n",
    "\n",
    "# recalc duration\n",
    "df[\"duration\"] = df[\"end_date\"]  - df[\"start_date\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedfd49",
   "metadata": {},
   "source": [
    "we've lost a few!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00366dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df['end_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be89889",
   "metadata": {},
   "source": [
    "# Statistics on bike usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd830e2",
   "metadata": {},
   "source": [
    "How many bikes we have in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bike_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73afa07",
   "metadata": {},
   "source": [
    "Look a statistics of bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_groups = df.groupby(\"bike_id\")\n",
    "\n",
    "# bike with the most trips\n",
    "group_counts = bike_groups.count()[\"filename\"] # pick abritrary column (without nulls) to get counts\n",
    "b_id = group_counts.idxmax()\n",
    "n_trips = group_counts.loc[b_id]\n",
    "\n",
    "print(f\"\"\"\n",
    "bike with most trips: {b_id}\n",
    "did {n_trips} trips\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d694c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike with the longest trips\n",
    "group_sums = bike_groups[\"duration\"].sum()\n",
    "b_id = group_sums.idxmax()\n",
    "d_sum = group_sums.loc[b_id]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "bike with longest sum duration of trips: {b_id}\n",
    "total of {d_sum} seconds\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe984f",
   "metadata": {},
   "source": [
    "how long are trips? (excluding outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = df[df[\"duration\"].dt.seconds < 10000][\"duration\"].dt.seconds.hist(bins=50)\n",
    "plt.xlabel('Trip duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Trip duration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f56a1",
   "metadata": {},
   "source": [
    "## Long-lived bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_per_bike = bike_groups[\"filename\"].count()\n",
    "bike_start = bike_groups[\"start_date\"].first()\n",
    "bike_end = bike_groups[\"end_date\"].last()\n",
    "\n",
    "bike_lifetime = bike_end - bike_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89502a93",
   "metadata": {},
   "source": [
    "how long is the lifetime of a bike? (in days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c68cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = bike_lifetime.dt.days.hist(bins=50)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Bikes')\n",
    "plt.title('Lifetime of a bike')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85694fa1",
   "metadata": {},
   "source": [
    "what is the average utilisation of a bike? (total ride duration / lifetime)\n",
    "\n",
    "this is the mean of the per bike utilisation. I.e. each bike's usage divided by it's total lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_sums = bike_groups[\"duration\"].sum()\n",
    "bike_utilisation = duration_sums / bike_lifetime\n",
    "bike_utilisation.mean()\n",
    "\n",
    "bike_utilisation.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = bike_utilisation.hist(bins=500)\n",
    "plt.xlim([0, 0.15])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Bikes')\n",
    "plt.title('Lifetime of a bike')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883089cd",
   "metadata": {},
   "source": [
    "### per month\n",
    "\n",
    "- how many bikes are \"alive\" by month?\n",
    "\n",
    "- how many stations are \"alive\" by month?\n",
    "\n",
    "- what is bike utilisation by month? \n",
    "\n",
    "our utilisation measure here will be slightly different to previous figure. Previously we looked at per bike utilisation and averaged this. Now, we're looking at sum of use over the entire fleet and dividing this by the max possible usage per month (24/7 riding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f312303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't want to incude first and last months as may be incompelte, use in filter later\n",
    "incomplete_months = df[\"start_date\"].iloc[[0, -1]].dt.to_period(\"M\")\n",
    "\n",
    "\n",
    "# create a complete monthly index that covers ALL months in period \n",
    "complete_monthly_index = pd.date_range(start=df[\"start_date\"].iloc[0], end=df[\"end_date\"].iloc[-1], freq=\"M\").to_period(\"M\")\n",
    "# remove incomplete months\n",
    "complete_monthly_index = complete_monthly_index.delete(complete_monthly_index.isin(incomplete_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO should stations count as allive for next month rather than current?\n",
    "def calc_alive_per_month(starts: pd.Series, ends: pd.Series, incomplete_months: pd.Series, complete_monthly_index: pd.PeriodIndex):\n",
    "    starts_per_month = starts.dt.to_period(\"M\").value_counts()\n",
    "    ends_per_month = ends.dt.to_period(\"M\").value_counts()\n",
    "\n",
    "    counts_df = complete_monthly_index.to_frame(name=\"foo\").join(starts_per_month).join(ends_per_month).sort_index().fillna(0)\n",
    "    # ending items should only be counted at the start of next month, so shift\n",
    "    counts_df[\"end_date\"] = counts_df[\"end_date\"].shift(fill_value=0)\n",
    "\n",
    "    alive_per_month = counts_df[\"start_date\"].cumsum() - counts_df[\"end_date\"].cumsum()\n",
    "    \n",
    "    return alive_per_month[~alive_per_month.index.isin(incomplete_months)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alive_bikes_per_month = calc_alive_per_month(starts=bike_start, ends=bike_end, incomplete_months=incomplete_months, complete_monthly_index=complete_monthly_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_sums_per_month = df[[\"duration\"]].groupby(df[\"start_date\"].dt.to_period(\"M\"))[\"duration\"].sum()\n",
    "\n",
    "duration_sums_per_month = duration_sums_per_month.to_frame()\n",
    "duration_sums_per_month[\"max_possible_duration\"] = duration_sums_per_month.index.map(lambda x: x.end_time - x.start_time)\n",
    "\n",
    "utilisation_per_month = duration_sums_per_month[\"duration\"] / duration_sums_per_month[\"max_possible_duration\"] / alive_bikes_per_month\n",
    "\n",
    "# remove incomplelte months\n",
    "utilisation_per_month = utilisation_per_month[~utilisation_per_month.index.isin(incomplete_months)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_groups = df.groupby(\"start_station_id\")\n",
    "\n",
    "# relies on time ordering of df via rental_id\n",
    "station_start = station_groups[\"start_date\"].first()\n",
    "station_end = station_groups[\"end_date\"].last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "alive_stations_per_month = calc_alive_per_month(starts=station_start, ends=station_end,\n",
    "                                                incomplete_months=incomplete_months, complete_monthly_index=complete_monthly_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf226dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill gaps\n",
    "stats_df = complete_monthly_index.to_frame(name=\"date\")\\\n",
    "    .join(alive_bikes_per_month.rename(\"alive_bikes\"))\\\n",
    "    .join(alive_stations_per_month.rename(\"alive_stations\"))\\\n",
    "    .join(utilisation_per_month.rename(\"utilisation\"))\\\n",
    "    .fillna(method=\"ffill\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd883f01",
   "metadata": {},
   "source": [
    "First month seems to be unusual, look from march 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ce1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[1:].plot.area(subplots=True)\n",
    "plt.xlabel('Years')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba41d9d",
   "metadata": {},
   "source": [
    "# chains\n",
    "\n",
    "A \"chain\" is a sequence of trips for a given bike, where the start location matches the previous end location.\n",
    "\n",
    "Run for some subset only. Try long lived bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_lived_bike_ids = bike_lifetime.sort_values()[-10:].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f70a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_bike_subset = df[df[\"bike_id\"].isin(top_ten_lived_bike_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chains(bike_id: int, bike_group: pd.DataFrame, df: pd.DataFrame) -> None:\n",
    "    \"\"\" note: adds to dataframe as side effect \"\"\"\n",
    "\n",
    "    # note fillna for end station to allow for comparison to NA\n",
    "    breaks = bike_group[bike_group[\"start_station_id\"] != bike_group.shift()[\"end_station_id\"].fillna(-1)]\n",
    "    break_indices = breaks.index.values\n",
    "\n",
    "    chains = list()\n",
    "    for i, (start, end) in enumerate(zip([None, *break_indices], [*break_indices, None])):\n",
    "        chain = bike_group.loc[start:end]\n",
    "        chain_id = f\"{bike_id}_{i}\"\n",
    "        chains.append(pd.Series(chain_id, index=chain.index))\n",
    "    return pd.concat(chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = list()\n",
    "for k, g in tqdm(top_ten_bike_subset.groupby(\"bike_id\")):\n",
    "    g = bike_groups.get_group(k)\n",
    "    chains.append(add_chains(bike_id=k, bike_group=g, df=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_bike_subset = top_ten_bike_subset.join(pd.concat(chains).rename(\"chain_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07635d",
   "metadata": {},
   "source": [
    "# Looking a behaviour of individual bikes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e447bd",
   "metadata": {},
   "source": [
    "- Take a look at the movement of some of these long lived bikes during a period of time\n",
    "\n",
    "- Clean dataset for places with missing stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc715e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_stations(df, stations):\n",
    "     \n",
    "    def check_id(row, stations):\n",
    "        start_id = str(int(row[\"start_station_id\"]))\n",
    "        end_id = str(int(row[\"end_station_id\"]))\n",
    "        if str(start_id) in stations.keys() and str(end_id) in stations.keys():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    df[\"check_stations_ids\"] = df.apply(\n",
    "        lambda row: check_id(row, stations), axis=1\n",
    "    )\n",
    "    df = df[df.check_stations_ids.eq(True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/BorisBikes_stations_coordinates.json\") as f:\n",
    "    stations = json.load(f)\n",
    "\n",
    "data = remove_missing_stations(top_ten_bike_subset,stations)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49840591",
   "metadata": {},
   "source": [
    "- Lets make this a bit more formal, by building objects htat represent bikes and trips\n",
    "\n",
    "- A trip is a journey between two stations, we can use https://www.cyclestreets.net/ api to build the most probable journey between two\n",
    "    point given the duration of that journey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db734e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip:\n",
    "    def __init__(self, data, bike_id, trip_id, station_data):\n",
    "        df = data[data.index == trip_id]\n",
    "\n",
    "        self.init_station = {\n",
    "            \"name\": df.start_station_name.values[0],\n",
    "            \"id\": df.start_station_id.values[0],\n",
    "            \"latitude\": station_data[str(int(df.start_station_id.values[0]))][\n",
    "                \"lat\"\n",
    "            ],\n",
    "            \"longitude\": station_data[str(int(df.start_station_id.values[0]))][\n",
    "                \"lon\"\n",
    "            ],\n",
    "        }\n",
    "        self.end_station = {\n",
    "            \"name\": df.end_station_name.values[0],\n",
    "            \"id\": df.end_station_id.values[0],\n",
    "            \"latitude\": station_data[str(int(df.end_station_id.values[0]))][\n",
    "                \"lat\"\n",
    "            ],\n",
    "            \"longitude\": station_data[str(int(df.end_station_id.values[0]))][\n",
    "                \"lon\"\n",
    "            ],\n",
    "        }\n",
    "        self.bike = df.bike_id.values[0]\n",
    "        self.duration = df.duration.values[0]\n",
    "        self.date = {\n",
    "            \"start\": df.start_date.values[0],\n",
    "            \"end\": df.end_date.values[0],\n",
    "        }\n",
    "        self.circular = self.init_station == self.end_station\n",
    "        self.route = {}\n",
    "        self.bike_id = bike_id\n",
    "        self.trip_id = trip_id\n",
    "\n",
    "    def get_route(self, key, route_path= 'routes/'):\n",
    "        \n",
    "        if not os.path.exists(route_path):\n",
    "            os.makedirs(route_path)\n",
    "            \n",
    "        route_file_path = (\n",
    "            route_path\n",
    "            + str(self.bike_id)\n",
    "            + \"_\"\n",
    "            + str(self.trip_id)\n",
    "            + \".json\"\n",
    "        )\n",
    "        if os.path.isfile(route_file_path):\n",
    "            with open(route_file_path, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                self.route = data\n",
    "        else:\n",
    "            if self.circular:\n",
    "                self.route = {}\n",
    "\n",
    "            else:\n",
    "                plans = [\"balanced\", \"fastest\", \"quietest\", \"shortest\"]\n",
    "\n",
    "                closest_time = False\n",
    "                trip_data = {}\n",
    "\n",
    "                for plan in plans:\n",
    "                    name = (\n",
    "                        \"https://www.cyclestreets.net/api/journey.json?key=\"\n",
    "                        + key\n",
    "                        + \"&itinerarypoints=\"\n",
    "                        + str(self.init_station[\"longitude\"])\n",
    "                        + \",\"\n",
    "                        + str(self.init_station[\"latitude\"])\n",
    "                        + \"|\"\n",
    "                        + str(self.end_station[\"longitude\"])\n",
    "                        + \",\"\n",
    "                        + str(self.end_station[\"latitude\"])\n",
    "                        + \"&plan=\"\n",
    "                        + plan\n",
    "                    )\n",
    "                    data = requests.get(name).json()[\"marker\"][0][\n",
    "                        \"@attributes\"\n",
    "                    ]\n",
    "                    time = int(data[\"time\"])\n",
    "                    if closest_time is False:\n",
    "                        closest_time = abs(time - self.duration)\n",
    "                        trip_data = data\n",
    "\n",
    "                    elif abs(self.duration - time) < closest_time:\n",
    "                        closest_time = abs(time - self.duration)\n",
    "                        trip_data = data\n",
    "\n",
    "                self.route = trip_data\n",
    "\n",
    "            with open(route_file_path, \"w\") as fp:\n",
    "                json.dump(self.route, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59239660",
   "metadata": {},
   "source": [
    "A bike is identified by its ID, and they story contains all the trips recorded in the data and the routed obtained from https://www.cyclestreets.net/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bike:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "\n",
    "    def get_chains(self, stations):\n",
    "        chain_ids = self.bike_rides.chain_id.to_list()\n",
    "        chains = {}\n",
    "        for chain_id in chain_ids:\n",
    "            chain_rides = self.bike_rides[\n",
    "                self.bike_rides[\"chain_id\"] == chain_id\n",
    "            ]\n",
    "            chains[chain_id] = [Trip(chain_rides, self.id, trip_id, stations) for trip_id in chain_rides.index]\n",
    "            #self.get_trips(chain_rides, stations)\n",
    "        self.chains = chains\n",
    "\n",
    "    def get_story(self, dataset, stations,key):\n",
    "        bike_rides = dataset[dataset[\"bike_id\"] == self.id]\n",
    "        self.bike_rides = bike_rides\n",
    "        self.get_chains(stations)\n",
    "        \n",
    "        for chain_id, chain in self.chains.items():\n",
    "\n",
    "            for counter, trip in enumerate(chain):\n",
    "                trip.get_route(key)\n",
    "                if trip.route == {}:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0be04",
   "metadata": {},
   "source": [
    "## A day of the life of a bike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4735f",
   "metadata": {},
   "source": [
    "We can visualise these journes of a given bike on a map using folium and moving pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d445acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colours(steps):\n",
    "    colours = sns.color_palette(\"mako\").as_hex()\n",
    "    rev_colours = sns.color_palette(\"mako\").as_hex()\n",
    "    rev_colours.reverse()\n",
    "    colours = rev_colours + colours\n",
    "    while len(colours) < steps:\n",
    "        colours += colours\n",
    "    return colours\n",
    "\n",
    "\n",
    "def get_trajectory(bike_id, route_folder = \"routes/\"):\n",
    "\n",
    "    chains = [\n",
    "        filename\n",
    "        for filename in sorted(os.listdir(route_folder))\n",
    "        if str(bike_id) + \"_\" in filename\n",
    "    ]\n",
    "    \n",
    "    times = []\n",
    "    geometry = []\n",
    "    colours = []\n",
    "\n",
    "    many_colurs = get_colours(len(chains))\n",
    "\n",
    "    for c in range(len(chains)):\n",
    "        chain = chains[c]\n",
    "        with open(route_folder + chain) as f:\n",
    "            d = json.load(f)\n",
    "        if len(d) > 0:\n",
    "            geometry += [\n",
    "                Point([float(y) for y in x.split(\",\")])\n",
    "                for x in d[\"coordinates\"].split(\" \")\n",
    "            ]\n",
    "            if len(times) == 0:\n",
    "                time_now = datetime.datetime.now()\n",
    "            else:\n",
    "                time_now = times[-1]\n",
    "            times += [\n",
    "                time_now + datetime.timedelta(seconds=1 * t + 1)\n",
    "                for t in range(len(d[\"coordinates\"].split(\" \")))\n",
    "            ]\n",
    "            colours += [\n",
    "                many_colurs[c] for x in range(len(d[\"coordinates\"].split(\" \")))\n",
    "            ]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df[\"t\"] = times\n",
    "    df[\"trajectory_id\"] = [1 for x in range(len(geometry))]\n",
    "    df[\"sequence\"] = [x + 1 for x in range(len(geometry))]\n",
    "    df[\"colour\"] = colours\n",
    "\n",
    "    gdf = GeoDataFrame(df, crs=\"EPSG:4326\", geometry=geometry)\n",
    "    gdf = gdf.set_index(\"t\")\n",
    "\n",
    "    trajs = mpd.TrajectoryCollection(gdf, \"trajectory_id\")\n",
    "    trajs = mpd.MinTimeDeltaGeneralizer(trajs).generalize(\n",
    "        tolerance=datetime.timedelta(seconds=1)\n",
    "    )\n",
    "    traj = trajs.trajectories[0]\n",
    "    return traj\n",
    "\n",
    "def draw_map(traj):\n",
    "    features = traj_to_timestamped_geojson(traj)\n",
    "    # Create base map\n",
    "    London = [51.506949, -0.122876]\n",
    "    map = folium.Map(location=London, zoom_start=12, tiles=\"cartodbpositron\")\n",
    "    TimestampedGeoJson(\n",
    "        {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": features,\n",
    "        },\n",
    "        period=\"PT1S\",\n",
    "        add_last_point=False,\n",
    "        transition_time=10,\n",
    "    ).add_to(map)\n",
    "    return map\n",
    "\n",
    "def traj_to_timestamped_geojson(trajectory):\n",
    "    features = []\n",
    "    df = trajectory.df.copy()\n",
    "    df[\"previous_geometry\"] = df[\"geometry\"].shift()\n",
    "    df[\"time\"] = df.index\n",
    "    df[\"previous_time\"] = df[\"time\"].shift()\n",
    "    for _, row in df.iloc[1:].iterrows():\n",
    "        coordinates = [\n",
    "            [\n",
    "                row[\"previous_geometry\"].xy[0][0],\n",
    "                row[\"previous_geometry\"].xy[1][0],\n",
    "            ],\n",
    "            [row[\"geometry\"].xy[0][0], row[\"geometry\"].xy[1][0]],\n",
    "        ]\n",
    "        times = [row[\"previous_time\"].isoformat(), row[\"time\"].isoformat()]\n",
    "        features.append(\n",
    "            {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"LineString\",\n",
    "                    \"coordinates\": coordinates,\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"times\": times,\n",
    "                    \"style\": {\n",
    "                        \"color\": row[\"colour\"],\n",
    "                        \"weight\": 5,\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = open(\"cycle_street_key.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_id = 893\n",
    "selected_data = data[(data['start_date']> '2020-03-23') & (data['start_date']< '2020-05-14')]\n",
    "bike = Bike(id=bike_id)\n",
    "bike.get_story(selected_data, stations,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = get_trajectory(bike_id)\n",
    "map_trajectory = draw_map(traj)\n",
    "\n",
    "map_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_id = 3278\n",
    "selected_data =data[(data['start_date']> '2021-03-23') & (data['start_date']< '2021-05-14')]\n",
    "bike = Bike(id=bike_id)\n",
    "bike.get_story(selected_data, stations,key)\n",
    "\n",
    "traj = get_trajectory(bike_id)\n",
    "map_trajectory = draw_map(traj)\n",
    "\n",
    "map_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c46d66",
   "metadata": {},
   "source": [
    "# Bike mobility patterns\n",
    "\n",
    "- Bikes seem to stay on the areas unless they get moved by car, which is not uncommon\n",
    "- Characterise the mobility patterns using network analysis.\n",
    "\n",
    "- Describe the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_from_data(df, trip_count_threshold = 1e-5):\n",
    "    trip_counts = (\n",
    "        (\n",
    "            df[[\"start_station_id\", \"end_station_id\", \"bike_id\"]]\n",
    "            .groupby([\"start_station_id\", \"end_station_id\"])\n",
    "            .count()\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"bike_id\": \"trip_count\"})\n",
    "    )\n",
    "    trip_counts = trip_counts.sort_values(\"trip_count\")\n",
    "    total_num_trips = trip_counts[\"trip_count\"].sum()\n",
    "\n",
    "    trip_counts = trip_counts[\n",
    "        trip_counts[\"trip_count\"] >= trip_count_threshold * total_num_trips\n",
    "    ]\n",
    "\n",
    "    graph = nx.from_pandas_edgelist(\n",
    "        trip_counts,\n",
    "        source=\"start_station_id\",\n",
    "        target=\"end_station_id\",\n",
    "        edge_attr=\"trip_count\",\n",
    "        create_using=nx.DiGraph,\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3486ac",
   "metadata": {},
   "source": [
    "describe community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfe385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_community_detection(graph, edge_weight):\n",
    "    graph_undirected = nx.Graph()\n",
    "    undirected_edges = set(sorted(graph.edges))\n",
    "    for edge in undirected_edges:\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        trip_count = graph.edges[edge][edge_weight]\n",
    "        if reverse_edge in graph.edges:\n",
    "            trip_count += graph.edges[reverse_edge][edge_weight]\n",
    "        graph_undirected.add_edge(edge[0], edge[1], trip_count=trip_count)\n",
    "\n",
    "    partition = community.best_partition(graph_undirected, weight=edge_weight)\n",
    "    df_partition = pd.DataFrame(partition, index=[0]).T.reset_index()\n",
    "    df_partition.columns = [\"id\", \"partition\"]\n",
    "\n",
    "    return df_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1779a9",
   "metadata": {},
   "source": [
    "Visualise the network, describe what we want to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb46fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "STATION_NAMES_FILE = \"../data/BorisBikes_station_names.pickle\"\n",
    "STATION_COORDS_FILE = LOCATION_REF\n",
    "\n",
    "LABEL_STATIONS = [\n",
    "     \"Belgrove Street\",\n",
    "     \"Waterloo Station 3\",\n",
    "     \"Hyde Park Corner\",\n",
    "     \"Aquatic Centre\",\n",
    "     \"Bethnal Green Road\",\n",
    "     \"Natural History Museum\",\n",
    "     \"Kennington Oval\",\n",
    "     \"Mudchute DLR\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_station_name(id):\n",
    "    with open(STATION_NAMES_FILE, \"rb\") as f:\n",
    "        station_allnames = pickle.load(f)\n",
    "\n",
    "    name = sorted(station_allnames[id])[0]\n",
    "    name = re.split(\";|,|:\", name)[0].strip()\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_node_info(graph):\n",
    "    with open(STATION_COORDS_FILE, \"r\") as f:\n",
    "        station_latlon = json.load(f)\n",
    "\n",
    "    nodes = graph.nodes()\n",
    "\n",
    "    pos = [station_latlon[str(int(node))] for node in nodes]\n",
    "    pos = [(p[\"lon\"], p[\"lat\"]) for p in pos]\n",
    "\n",
    "    station_sizes = [i[1] for i in list(graph.degree(weight=\"trip_count\"))]\n",
    "\n",
    "    labels = [get_station_name(int(node)) for node in nodes]\n",
    "\n",
    "    nodes_df = pd.DataFrame(\n",
    "        {\"id\": list(nodes), \"pos\": pos, \"size\": station_sizes, \"name\": labels}\n",
    "    )\n",
    "\n",
    "    return nodes_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _scale_range(values, min_scaled, max_scaled):\n",
    "    values = np.array(values)\n",
    "    if min_scaled is not None:\n",
    "        max_value = np.max(values)\n",
    "        min_value = np.min(values)\n",
    "        mult_coeff = (max_scaled - min_scaled) / (max_value - min_value)\n",
    "        add_coeff = (max_value * min_scaled - min_value * max_scaled) / (\n",
    "            max_value - min_value\n",
    "        )\n",
    "        scaled = mult_coeff * values + add_coeff\n",
    "    else:\n",
    "        max_value = np.max(values)\n",
    "        scaled = max_scaled * values / max_value\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def _drop_stations_without_location(graph):\n",
    "    with open(STATION_COORDS_FILE, \"r\") as f:\n",
    "        station_latlon = json.load(f)\n",
    "    nodes = tuple(graph.nodes)\n",
    "    stations_with_location = tuple(map(int, station_latlon.keys()))\n",
    "    for n in nodes:\n",
    "        if n not in stations_with_location:\n",
    "            print(f\"Removing node {n} because of missing location data.\")\n",
    "            graph.remove_node(n)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_network_and_map(\n",
    "    df,\n",
    "    label_stations= LABEL_STATIONS,\n",
    "    allow_self_loops=False,\n",
    "    arrows=True,\n",
    "):\n",
    "    community_graph = create_network_from_data(df)\n",
    "    _drop_stations_without_location(community_graph)\n",
    "    nodes_info = get_node_info(community_graph)\n",
    "    visualisation_graph = community_graph.copy()\n",
    "    if not allow_self_loops:\n",
    "        visualisation_graph.remove_edges_from(\n",
    "            nx.selfloop_edges(community_graph)\n",
    "        )\n",
    "    community_df = network_community_detection(community_graph, \"trip_count\")\n",
    "    nodes_info = nodes_info.merge(community_df, on=\"id\")\n",
    "    nodes_info = nodes_info.sort_values(by=\"size\", ascending=False)\n",
    "    del community_df\n",
    "\n",
    "    nodes_info['lon'] = [p[0] for p in nodes_info[\"pos\"]]\n",
    "    nodes_info['lat'] = [p[1] for p in nodes_info[\"pos\"]]\n",
    "\n",
    "    nodes_info = GeoDataFrame(nodes_info,geometry=points_from_xy(nodes_info.lon, nodes_info.lat),crs=\"EPSG:4326\")\n",
    "    \n",
    "    labels = {\n",
    "        id: name\n",
    "        for id, name in zip(nodes_info[\"id\"], nodes_info[\"name\"])\n",
    "        if name in label_stations\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        1, 1, figsize=(20, 15)\n",
    "    )\n",
    "    nodes_info.plot(ax=ax,markersize=1)\n",
    "    cx.add_basemap(ax,crs=nodes_info.crs,source=cx.providers.Stamen.TonerLite)\n",
    "\n",
    "    xynps = [np.array([p[0] for p in nodes_info[\"pos\"]]),\n",
    "        np.array([p[1] for p in nodes_info[\"pos\"]])]\n",
    "    pos = {\n",
    "        k: (xynps[0][i], xynps[1][i]) for i, k in enumerate(nodes_info[\"id\"])\n",
    "    }\n",
    "\n",
    "    MAX_NODE_SIZE = 300.0\n",
    "    MIN_NODE_SIZE = 5.0\n",
    "    \n",
    "    sizes = _scale_range(nodes_info[\"size\"], MIN_NODE_SIZE, MAX_NODE_SIZE)\n",
    "    weights = np.array(\n",
    "        [\n",
    "            visualisation_graph.edges[e][\"trip_count\"]\n",
    "            for e in visualisation_graph.edges\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    MAX_EDGE_WIDTH = 3.0\n",
    "    MIN_EDGE_WIDTH = None\n",
    "    \n",
    "    \n",
    "    weights = _scale_range(weights, MIN_EDGE_WIDTH, MAX_EDGE_WIDTH)\n",
    "    \n",
    "    MAX_EDGE_ALPHA = 0.9\n",
    "    MIN_EDGE_ALPHA = None\n",
    "\n",
    "    edge_alpha = _scale_range(weights, MIN_EDGE_ALPHA, MAX_EDGE_ALPHA)\n",
    "\n",
    "    \n",
    "    # Plots\n",
    "    nx.draw_networkx_nodes(\n",
    "        visualisation_graph,\n",
    "        pos=pos,\n",
    "        nodelist=nodes_info[\"id\"],\n",
    "        node_color=nodes_info[\"partition\"],\n",
    "        alpha=1.0,\n",
    "        node_size=sizes,\n",
    "        cmap=\"tab10\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    nx.draw_networkx_edges(\n",
    "        visualisation_graph,\n",
    "        pos=pos,\n",
    "        edge_color=\"#222222\",\n",
    "        width=weights,\n",
    "        alpha=edge_alpha,\n",
    "        arrows=arrows,\n",
    "        ax=ax,\n",
    "    )\n",
    "    nx.draw_networkx_labels(\n",
    "        visualisation_graph,\n",
    "        pos=pos,\n",
    "        labels=labels,\n",
    "        font_size=12,\n",
    "        ax=ax,\n",
    "    )\n",
    "    return fig, ax, nodes_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adceb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "HARD_START_DATE = datetime.datetime(year=2010, month=1, day=1)\n",
    "\n",
    "start_date = datetime.datetime(year=2021, month=1, day=1)\n",
    "end_date = datetime. datetime(year=2022, month=1, day=1)\n",
    "\n",
    "df = df[\n",
    "    (df[\"start_date\"] > HARD_START_DATE) & (df[\"end_date\"] > HARD_START_DATE)\n",
    "]\n",
    "df_year = df[(df[\"start_date\"] > start_date) & (df[\"start_date\"] < end_date)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52c7c3",
   "metadata": {},
   "source": [
    "Take a look at the network on a tipical morning on 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58643427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting mornings\")\n",
    "df_year_mornings = df[\n",
    "    df[\"start_date\"].dt.hour.isin([7, 8, 9, 10])\n",
    "    & df[\"start_date\"].dt.weekday.isin((0, 1, 2, 3, 4))\n",
    "]\n",
    "fig, ax, nodes_info = create_network_and_map(df_year_mornings)\n",
    "num_communities = len(nodes_info[\"partition\"].unique())\n",
    "print(f\"Number of communities: {num_communities}\")\n",
    "plt.title(\"Weekday mornings (7-10)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93776e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Take a look at the network on a tipical afternoon on 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955825ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting afternoons\")\n",
    "df_year_afternoons = df[\n",
    "    df[\"start_date\"].dt.hour.isin([15, 16, 17, 18, 19])\n",
    "    & df[\"start_date\"].dt.weekday.isin((0, 1, 2, 3, 4))\n",
    "]\n",
    "fig, ax, nodes_info = create_network_and_map(df_year_afternoons)\n",
    "num_communities = len(nodes_info[\"partition\"].unique())\n",
    "print(f\"Number of communities: {num_communities}\")\n",
    "plt.title(\"Weekday afternoons (15-19)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee06707",
   "metadata": {},
   "outputs": [],
   "source": [
    "Take a look at the network on a tipical weekend on 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting weekends\")\n",
    "df_year_weekends = df[df[\"start_date\"].dt.weekday.isin((5, 6))]\n",
    "fig, ax, nodes_info = create_network_and_map(\n",
    "    df_year_weekends,\n",
    "    allow_self_loops=True,\n",
    ")\n",
    "num_communities = len(nodes_info[\"partition\"].unique())\n",
    "print(f\"Number of communities: {num_communities}\")\n",
    "plt.title(\"Weekends\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3815cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets see how the patterns have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in (2013, 2017, 2020,2021):\n",
    "    print(f\"Plotting {year}\")\n",
    "    start_date = datetime.datetime(year=year, month=1, day=1)\n",
    "    end_date = datetime.datetime(year=year + 1, month=1, day=1)\n",
    "    df_year = df[\n",
    "        (df[\"start_date\"] > start_date) & (df[\"start_date\"] < end_date)\n",
    "    ]\n",
    "    fig, ax, nodes_info = create_network_and_map(\n",
    "        df_year,\n",
    "        allow_self_loops=False,\n",
    "        arrows=False,\n",
    "    )\n",
    "    num_communities = len(nodes_info[\"partition\"].unique())\n",
    "    print(f\"Number of communities: {num_communities}\")\n",
    "    plt.title(f\"Year {year}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6dc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfeffel_poetry",
   "language": "python",
   "name": "pfeffel_poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
