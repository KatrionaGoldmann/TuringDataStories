{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eurovision Song Contest Data\n",
    "\n",
    "This notebook imports all the data required to visualise and model trends in the Eurovision voting.\n",
    "The data imported is from the following sources:\n",
    "\n",
    "- Voting Scores\n",
    "    - 1975-2019 data from [Kaggle](https://www.kaggle.com/datasets/datagraver/eurovision-song-contest-scores-19752019)\n",
    "    - 2020 was cancelled\n",
    "    - 2021, 2022 scraped from Wikipedia\n",
    "    - We limit to 1998 onwards because this is when the voting changed to include tele-voting.\n",
    "- Song and country languages\n",
    "    - Performance language from [Kaggle](https://www.kaggle.com/datasets/minitree/eurovision-song-lyrics?select=eurovision-lyrics-2022.json)\n",
    "    - Official country language from [wikipedia](https://en.wikipedia.org/wiki/List_of_official_languages_by_country_and_territory)\n",
    "- Gender\n",
    "    - The gender data is guessed by scraping the wikipedia page for the performing artist\n",
    "- Migration data between performing and voting countries\n",
    "    - [Our World in Data](https://ourworldindata.org/migration) on international migration, under the 'Explore data on where people migrate from and to' section.\n",
    "       Original source is from the UN. Data shows total number of immigrants in each country split by country of origin in the years 1990-2020, recorded at intervals of every 5 years.\n",
    "    - Country populations are from the [World Bank](https://data.worldbank.org/indicator/SP.POP.TOTL?end=2021&start=2021&view=map).\n",
    "- Country borders\n",
    "    - [GeoDataSource](https://github.com/geodatasource/country-borders/)   \n",
    "- Competition winners\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/List_of_Eurovision_Song_Contest_winners)\n",
    "- 2023 performers\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/Eurovision_Song_Contest_2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting scores\n",
    "\n",
    "We import the voting scores from a variety of sources: \n",
    " - 1975-2019 data from Kaggle: https://www.kaggle.com/datasets/datagraver/eurovision-song-contest-scores-19752019\n",
    " - 2020 was cancelled\n",
    " - 2021, 2022 scraped from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pycountry\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data from the kaggle dataset\n",
    "votes_1975_2019 = pd.read_excel(\"data/eurovision_song_contest_1975_2019.xlsx\")\n",
    "\n",
    "# Clean up column names first\n",
    "votes_1975_2019.columns = [c.strip().lower().replace(' ', '_') for c in votes_1975_2019.columns.values.tolist()]\n",
    "\n",
    "print(votes_1975_2019.shape)\n",
    "votes_1975_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up this dataset.\n",
    "\n",
    "# Select only finals votes, and only 1998 onwards (inclusive)\n",
    "votes_1998_2019 = votes_1975_2019[(votes_1975_2019['(semi-)_final'] == 'f') & (votes_1975_2019['year'] >= 1998)]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "votes_1998_2019 = votes_1998_2019[[\"year\", \"from_country\", \"to_country\", \"points\", \"jury_or_televoting\"]]\n",
    "\n",
    "# Clean up country names\n",
    "def standardise_country(c):\n",
    "    replacements = [('-', ' '), ('&', 'and'), ('netherands', 'netherlands'),\n",
    "                    # FYR Macedonia was formally renamed as North Macedonia in 2019\n",
    "                    ('f.y.r. macedonia', 'north macedonia'), \n",
    "                    ('russia', 'russian federation'), \n",
    "                    ('the netherlands', 'netherlands'), \n",
    "                    ('czech republic', 'czechia'),\n",
    "                    # Yugoslavia dissolved in 2002; most of it became 'Serbia and Montenegro', until 2006, when Serbia and Montenegro split ways.\n",
    "                    ('serbia and montenegro', 'yugoslavia'),\n",
    "                    ('moldova', 'moldova, republic of')]\n",
    "    c = c.lower()\n",
    "    for r in replacements:\n",
    "        c = c.replace(r[0], r[1])\n",
    "    return c\n",
    "\n",
    "for column in ['from_country', 'to_country']:\n",
    "    votes_1998_2019[column] = votes_1998_2019[column].map(standardise_country)\n",
    "\n",
    "# Drop columns which correspond to the same vote (there are two Belarus -> Russia in 2019, for example)\n",
    "votes_1998_2019 = votes_1998_2019.drop_duplicates(subset=['year', 'from_country', 'to_country', 'jury_or_televoting'])\n",
    "\n",
    "# Drop Lithuania in 2003 (they didn't participate - I don't know why it's still in the dataset)\n",
    "votes_1998_2019 = votes_1998_2019[~((votes_1998_2019['to_country'] == 'lithuania') & (votes_1998_2019['year'] == 2003))]\n",
    "\n",
    "# Drop \"votes\" from one country to itself\n",
    "votes_1998_2019 = votes_1998_2019[votes_1998_2019['from_country'] != votes_1998_2019['to_country']]\n",
    "\n",
    "votes_1998_2019.sample(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to fetch some data from Wikipedia for the 2021 and 2022 contests.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def import_votes_from_wp(year: int) -> pd.DataFrame:\n",
    "    # ID numbers for the respective tables on the Wikipedia page.\n",
    "    JURY_ID = 16\n",
    "    TELEVOTING_ID = 17\n",
    "\n",
    "    url = f\"https://en.wikipedia.org/wiki/Eurovision_Song_Contest_{year}#Final_2\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table', {'class': \"wikitable\"})\n",
    "\n",
    "    def parse_table_from_id(wp_id: int, jury_or_tele: str) -> pd.DataFrame:\n",
    "        df_table = pd.read_html(str(tables[wp_id]))\n",
    "        df_table = pd.DataFrame(df_table[0])\n",
    "\n",
    "        # remove redundant rows/columns\n",
    "        df_table = df_table.drop(df_table.columns[[0, 2, 3, 4]], axis=1)\n",
    "        df_table = df_table.drop(df_table.index[[0, 2]], axis=0) \n",
    "\n",
    "        # set the index to the first column\n",
    "        df_table = df_table.set_index(df_table.columns[0])\n",
    "\n",
    "        # set the column names as the first row\n",
    "        df_table.columns = df_table.iloc[0]\n",
    "        df_table = df_table.drop(df_table.index[0])\n",
    "\n",
    "        # replace NaN with 0\n",
    "        df_table = df_table.fillna(0)\n",
    "\n",
    "        # squash the column index with stack\n",
    "        df_table = df_table.stack().reset_index()\n",
    "\n",
    "        df_table.columns = ['to_country', 'from_country', 'points']\n",
    "        df_table['jury_or_televoting'] = jury_or_tele\n",
    "\n",
    "        df_table['year'] = year\n",
    "\n",
    "        # re-order the columns to match the original data   \n",
    "        df_table = df_table[['year', 'from_country', 'to_country', 'points', 'jury_or_televoting']]\n",
    "        \n",
    "        df_table['points'] = df_table['points'].astype(int)\n",
    "        \n",
    "        # Clean up countries as before\n",
    "        for column in ['from_country', 'to_country']:\n",
    "            df_table[column] = df_table[column].map(standardise_country)\n",
    "\n",
    "        return(df_table)\n",
    "\n",
    "    jury_table = parse_table_from_id(JURY_ID, jury_or_tele='J')\n",
    "    tele_table = parse_table_from_id(TELEVOTING_ID, jury_or_tele='T')\n",
    "    return(pd.concat([jury_table, tele_table]))\n",
    "\n",
    "votes_1998_2022 = pd.concat([votes_1998_2019,\n",
    "                             import_votes_from_wp(2021),\n",
    "                             import_votes_from_wp(2022)])\n",
    "\n",
    "# Again, drop \"votes\" from one country to herself\n",
    "votes_1998_2022 = votes_1998_2022[votes_1998_2022['from_country'] != votes_1998_2022['to_country']]\n",
    "\n",
    "votes_1998_2022.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is a sanity check to make sure that all countries participating in \n",
    "# a given year got the same number of votes. We hope to see the 'is_consistent' \n",
    "# column be True for all years in the output.\n",
    "\n",
    "def check_consistency(df):\n",
    "    def all_entries_same(arr : np.ndarray) -> bool:\n",
    "        # Determines if all non-NaN entries in a numpy array have the same value.\n",
    "        arr2 = arr[~np.isnan(arr)]\n",
    "        return np.all(arr2 == arr2[0])\n",
    "\n",
    "    # Pivot to wide form, so that each row gives the number of scores each country received in a given year\n",
    "    grouped_votes = df.groupby(by=['year', 'to_country'])['points'].count().reset_index()\n",
    "    grouped_votes = grouped_votes.pivot(index=\"year\", columns=\"to_country\", values=\"points\")\n",
    "    # Create \"is_consistent\" column and move it to the front\n",
    "    col_names = grouped_votes.columns\n",
    "    grouped_votes[\"is_consistent\"] = grouped_votes.apply(all_entries_same, axis=1, raw=True)\n",
    "    new_col_names = [\"is_consistent\", *col_names]\n",
    "\n",
    "    if(not all(grouped_votes[\"is_consistent\"])):     \n",
    "        # Show data\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            display(grouped_votes.reindex(columns=new_col_names))\n",
    "\n",
    "        raise Exception(\"Inconsistent number of votes received by countries in some years!\")\n",
    "    else: \n",
    "        print(\"All years are consistent!\")\n",
    "        \n",
    "check_consistency(votes_1998_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to combine jury and televoting scores.\n",
    "\n",
    "# Years where jury voting happened\n",
    "jury_years = np.unique(votes_1998_2022[votes_1998_2022['jury_or_televoting'] == 'J']['year'])\n",
    "# Years where televoting happened\n",
    "televoting_years = np.unique(votes_1998_2022[votes_1998_2022['jury_or_televoting'] == 'T']['year'])\n",
    "# Years where both happened (i.e. the intersection)\n",
    "double_voting_years = np.intersect1d(jury_years, televoting_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the years for which the points can just be used as-is.\n",
    "votes_to_keep = votes_1998_2022[~votes_1998_2022['year'].isin(double_voting_years)]\n",
    "votes_to_keep = votes_to_keep.drop(columns=['jury_or_televoting'])\n",
    "\n",
    "# These are the years which we need to process.\n",
    "# The way we do this is to add up the J and T scores, then re-rank them and assign 12 points to the highest score, 10 to the next-highest, etc.\n",
    "votes_to_process = votes_1998_2022[votes_1998_2022['year'].isin(double_voting_years)]\n",
    "summed_votes = votes_to_process.sort_values(by=['year', 'from_country', 'to_country'])\n",
    "summed_votes = summed_votes.groupby(by=['year', 'from_country', 'to_country']).sum(numeric_only=True)\n",
    "\n",
    "def rescale_points(pts: pd.Series) -> pd.Series:\n",
    "    # grp is a pd.Series corresponding to one combination of 'year' and 'from_country'\n",
    "    ranks_to_rescaled_points = {1: 12, 2: 10, 3: 8, 4: 7, 5: 6, 6: 5, 7: 4, 8: 3, 9: 2, 10: 1}\n",
    "    ranks = [sorted(pts, reverse=True).index(pt) + 1 for pt in pts]\n",
    "    rescaled_points = {pt: ranks_to_rescaled_points.get(r, 0) for pt, r in zip(pts, ranks)}\n",
    "    return pts.map(rescaled_points)\n",
    "\n",
    "processed_votes = summed_votes.groupby(by=['year', 'from_country']).transform(rescale_points).reset_index()\n",
    "processed_votes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x = processed_votes.rename(columns={\"points\": \"rescaled\"})\n",
    "x = x.set_index([\"year\", \"from_country\", \"to_country\"])\n",
    "v = summed_votes.reset_index().set_index([\"year\", \"from_country\", \"to_country\"])\n",
    "joined = v.join(x, how=\"outer\").reset_index()\n",
    "joined[(joined['year'] == 2016) & (joined['from_country'] == 'albania')].sort_values(by=\"points\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in country codes, and that's our final voting data.\n",
    "votes = pd.concat([votes_to_keep, processed_votes]).reset_index(drop=True)\n",
    "\n",
    "def get_country_codes(name):\n",
    "    if name == 'yugoslavia':\n",
    "        # That's how it's encoded in pycountry.\n",
    "        # https://github.com/flyingcircusio/pycountry/blob/main/src/pycountry/databases/iso3166-3.json\n",
    "        cty = pycountry.historic_countries.get(name='yugoslavia, socialist federal republic of')\n",
    "    else:\n",
    "        cty = pycountry.countries.get(name=name)\n",
    "    if cty is None:\n",
    "        raise KeyError(\"Country name \" + name + \" not found in pycountry. This really shouldn't happen.\")\n",
    "    \n",
    "    return cty.alpha_2, cty.alpha_3\n",
    "\n",
    "for ft in ['from', 'to']:\n",
    "    votes[f'{ft}_code2'], votes[f'{ft}_code3'] = zip(*votes[f'{ft}_country'].map(get_country_codes))\n",
    "\n",
    "# Add column for each country and year get the total number of points received\n",
    "votes['total_points'] = votes.groupby(by=['year', 'to_country'])['points'].transform('sum')\n",
    "\n",
    "# For each year rank the countries by total points received, where draws get same value\n",
    "temp = votes[['year', 'to_country', 'total_points']].drop_duplicates()\n",
    "temp['rank'] = temp.groupby(by=['year'])['total_points'].rank(method='first', ascending=False)\n",
    "\n",
    "# merge votes with ranks\n",
    "votes = votes.merge(temp, on=['year', 'to_country', 'total_points'], how='left')\n",
    "\n",
    "\n",
    "votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the numbers\n",
    "temp = votes[['from_country', 'year']].value_counts()\n",
    "\n",
    "# for each year print the unique values\n",
    "for year, group in temp.groupby(level=1):\n",
    "    print(year, group.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_consistency(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "votes.to_csv('data/votes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song language\n",
    "\n",
    "- Performance language from Kaggle: https://www.kaggle.com/datasets/minitree/eurovision-song-lyrics?select=eurovision-lyrics-2022.json\n",
    "- Official country language from wikipedia: https://en.wikipedia.org/wiki/List_of_official_languages_by_country_and_territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_json('data/eurovision-lyrics-2022.json').T\n",
    "songs = songs[['Country', 'Artist', 'Language', 'Year']]\n",
    "\n",
    "# Rename a couple of columns\n",
    "songs = songs.rename(columns={'Language': 'Language_sung'})\n",
    "\n",
    "# Tidy up country names\n",
    "for original, replacement in [('Macedonia', 'North Macedonia'),\n",
    "                              ('Russia', 'russian federation'),\n",
    "                              ('Serbia and Montenegro', 'yugoslavia'),\n",
    "                              ('Moldova', 'moldova, republic of'),\n",
    "                              ('Czech Republic', 'czechia'),\n",
    "                              ('The Netherlands', 'netherlands')]:\n",
    "    songs.loc[songs['Country'] == original, 'Country'] = replacement\n",
    "songs['Country'] = songs['Country'].str.lower()\n",
    "\n",
    "# Limit to 1998 and later\n",
    "songs['Year'] = pd.to_numeric(songs['Year'])\n",
    "songs = songs[songs['Year'] > 1997]\n",
    "\n",
    "# Add country code columns\n",
    "songs['Country_code2'], songs['Country_code3'] = zip(\n",
    "    *songs['Country'].map(get_country_codes))\n",
    "\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy up the language sung column\n",
    "songs['Language_sung'] = songs['Language_sung'].str.lower()\n",
    "songs['Language_sung'] = songs['Language_sung'].str.replace('partly|dialect|title|and', '', regex=True)\n",
    "\n",
    "# for each key in the dictionary, replace the value with the key\n",
    "replace_strings = {\n",
    "    'fr\\\\.': 'french', 'eng\\\\.': 'english', 'gr\\\\.': 'greek', \n",
    "    'sp\\\\.': 'spanish', 'rom\\\\.': 'romanian', 'russ\\\\.': 'russian',\n",
    "    'it\\\\.': 'italian', 'germ\\\\.': 'german', 'pol\\\\.': 'polish', \n",
    "    'sign language': 'sign-language'\n",
    "}\n",
    "\n",
    "for key, value in replace_strings.items():\n",
    "    songs['Language_sung'] = songs['Language_sung'].str.replace(key, value, regex=True)\n",
    "\n",
    "def extract_languages(lang_string):\n",
    "    \"\"\"Convert the string in language_sung into a list of languages\"\"\"\n",
    "    langs = re.split(r'\\s*[/()]\\s*', lang_string)\n",
    "    langs = [lang.strip() for lang in langs]\n",
    "    return [lang for lang in langs if lang != \"\"]\n",
    "\n",
    "songs['Language_sung'] = songs['Language_sung'].apply(extract_languages)\n",
    "songs.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs['Contains_English'] = songs['Language_sung'].apply(lambda x: 'english' in x)\n",
    "songs['Contains_NonEnglish'] = songs['Language_sung'].apply(lambda x: x != ['english'])\n",
    "\n",
    "songs[['Contains_English', 'Contains_NonEnglish']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of songs containing English or non-English:')\n",
    "\n",
    "songs[['Contains_English', 'Contains_NonEnglish']].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to see whether countries are singing in their official language. We can get the official language from [Wikipedia](https://en.wikipedia.org/wiki/List_of_official_languages_by_country_and_territory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the official languages from Wikipedia\n",
    "\n",
    "url = (\n",
    "    f\"https://en.wikipedia.org/wiki/List_of_official_languages_by_country_and_territory\"\n",
    ")\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "tables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "table = tables[0]\n",
    "df_languages = pd.read_html(str(table))\n",
    "df_languages = pd.DataFrame(df_languages[0])\n",
    "\n",
    "# Tidy the columns\n",
    "df_languages = df_languages.fillna(\"\")\n",
    "df_languages[\"Country/Region\"] = df_languages[\"Country/Region\"].apply(\n",
    "    lambda x: re.sub(\"\\[.*?\\]\", \"\", x)\n",
    ")\n",
    "df_languages.rename(columns={\"Official language\": \"Official_languages\"}, inplace=True)\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].apply(\n",
    "    lambda x: re.sub(\"\\[.*?\\]\", \"\", x)\n",
    ")\n",
    "\n",
    "# Tidy the country names\n",
    "df_languages[\"Country/Region\"] = df_languages[\"Country/Region\"].str.lower()\n",
    "\n",
    "df_languages.loc[\n",
    "    df_languages[\"Country/Region\"] == \"united kingdom and crown dependencies etc.\",\n",
    "    \"Country/Region\",\n",
    "] = \"united kingdom\"\n",
    "df_languages.loc[\n",
    "    df_languages[\"Country/Region\"] == \"russia\", \"Country/Region\"\n",
    "] = \"russian federation\"\n",
    "df_languages.loc[\n",
    "    df_languages[\"Country/Region\"] == \"serbia and montenegro\", \"Country/Region\"\n",
    "] = \"yugoslavia\"\n",
    "df_languages.loc[\n",
    "    df_languages[\"Country/Region\"] == \"moldova\", \"Country/Region\"\n",
    "] = \"moldova, republic of\"\n",
    "df_languages.loc[\n",
    "    df_languages[\"Country/Region\"] == \"czech republic\", \"Country/Region\"\n",
    "] = \"czechia\"\n",
    "df_languages = pd.concat(\n",
    "    [\n",
    "        df_languages,\n",
    "        pd.Series(\n",
    "            {\n",
    "                \"Country/Region\": \"yugoslavia\",\n",
    "                \"Official_languages\": \"serbian montenegrin\",\n",
    "            }\n",
    "        )\n",
    "        .to_frame()\n",
    "        .T,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "set(songs[\"Country\"].unique()) - set(df_languages[\"Country/Region\"].unique())\n",
    "\n",
    "df_languages = df_languages.loc[df_languages[\"Country/Region\"].isin(songs[\"Country\"].unique())]\n",
    "\n",
    "df_languages[\"Country_code2\"], df_languages[\"Country_code3\"] = zip(\n",
    "    *df_languages[\"Country/Region\"].map(get_country_codes)\n",
    ")\n",
    "\n",
    "# Tidy the language column\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].str.lower()\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].apply(\n",
    "    lambda x: x.replace(\"all have de facto status\", \"\")\n",
    ")\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].apply(\n",
    "    lambda x: x.replace(\",\", \"\")\n",
    ")\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].apply(\n",
    "    lambda x: x.replace(\"(\", \"\")\n",
    ")\n",
    "df_languages[\"Official_languages\"] = df_languages[\"Official_languages\"].apply(\n",
    "    lambda x: x.replace(\")\", \"\")\n",
    ")\n",
    "\n",
    "# Manually add missing languages\n",
    "df_languages.loc[df_languages[\"Country_code2\"] == \"LT\", \"Official_languages\"] = (\n",
    "    \"samogitian \"\n",
    "    + df_languages.loc[df_languages[\"Country_code2\"] == \"LT\", \"Official_languages\"]\n",
    ")\n",
    "df_languages.loc[df_languages[\"Country_code2\"] == \"FR\", \"Official_languages\"] = (\n",
    "    \"breton corsican \"\n",
    "    + df_languages.loc[df_languages[\"Country_code2\"] == \"FR\", \"Official_languages\"]\n",
    ")\n",
    "df_languages.loc[df_languages[\"Country_code2\"] == \"SI\", \"Official_languages\"] = (\n",
    "    \"slovenian \" + df_languages.loc[df_languages[\"Country_code2\"] == \"SI\", \"Official_languages\"]\n",
    ")\n",
    "df_languages.loc[df_languages[\"Country_code2\"] == \"EE\", \"Official_languages\"] = (\n",
    "    \"võro \" + df_languages.loc[df_languages[\"Country_code2\"] == \"EE\", \"Official_languages\"]\n",
    ")\n",
    "\n",
    "df_languages.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print any countries in songs['Country'] that are not in df_languages['Country/Region']\n",
    "if len(set(songs['Country_code2']) - set(df_languages['Country_code2'])) > 0: \n",
    "    countries = list(set(songs['Country_code2']) - set(df_languages['Country_code2']))\n",
    "    raise KeyError(\"Country name \" + ', '.join(countries) + \" was in songs, but not in df_languages.\")\n",
    "\n",
    "# merge df_languages and language on Country and Country/Region\n",
    "songs = pd.merge(songs, df_languages[['Country_code2', 'Official_languages']], left_on='Country_code2', right_on='Country_code2', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy the languages column\n",
    "songs['Official_languages'] = songs['Official_languages'].fillna(' ')\n",
    "\n",
    "# Add more columns\n",
    "def get_n_languages(langs):\n",
    "    \"\"\"Get the number of languages in a list of languages\"\"\"\n",
    "    if '6 other' in langs:   # [\"english\", \"6 other\"] -> 7\n",
    "        return len(langs) + 5\n",
    "    elif '10 other' in langs:\n",
    "        return len(langs) + 9\n",
    "    else:\n",
    "        return len(langs)\n",
    "songs['Contains_Multiple_Languages'] = songs['Language_sung'].apply(lambda x: len(x) > 1)\n",
    "songs['Number_of_Languages'] = songs['Language_sung'].apply(get_n_languages)\n",
    "songs['Contains_Own_Language'] = songs.apply(lambda df: len(set(df['Language_sung']).intersection(df['Official_languages'].split())) > 0, axis=1)\n",
    "\n",
    "songs[songs['Number_of_Languages'] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether the song is performed in the language of the voting country\n",
    "df_voting_language = df_languages.copy()\n",
    "df_voting_language['Voting_Languages'] = df_voting_language['Official_languages']\n",
    "\n",
    "votes = pd.merge(votes, df_voting_language[['Country_code2', 'Voting_Languages']], \n",
    "                 left_on='from_code2', right_on='Country_code2', how='left')\n",
    "\n",
    "# Combine votes and language\n",
    "df_VL = pd.merge(votes, songs, left_on=['to_code2', 'year'], right_on=['Country_code2', 'Year'], how='left')\n",
    "df_VL['Contains_Voting_Language'] = df_VL.apply(lambda df: len(set(df['Language_sung']).intersection(df['Voting_Languages'].split())) > 0, axis=1)\n",
    "\n",
    "df_VL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Country and to_country are identical\n",
    "if not all([all(df_VL['Country'] == df_VL['to_country']),\n",
    "            all(df_VL['Country_code2_x'] == df_VL['from_code2']),\n",
    "            all(df_VL['Country_code2_y'] == df_VL['to_code2']),\n",
    "            all(df_VL['Year'] == df_VL['year'])]):\n",
    "    raise ValueError(\"Mismatch in the merge - check this out!\")\n",
    "\n",
    "df_VL = df_VL[[\n",
    "    'year', 'Artist',\n",
    "    'from_country',\t'to_country', 'points', 'total_points', \n",
    "    'rank',\t'from_code2', 'from_code3', 'to_code2', 'to_code3',\n",
    "    'Official_languages', 'Language_sung',\n",
    "    'Contains_English', 'Contains_NonEnglish', 'Contains_Multiple_Languages',\n",
    "    'Number_of_Languages', 'Contains_Own_Language', 'Contains_Voting_Language']]\n",
    "\n",
    "df_VL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_VL.to_csv('data/language.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performer gender\n",
    "\n",
    "We determine the artists gender by scraping the wikipedia page for the performing artist. Artists are classed as either male, female or group. \n",
    "\n",
    "Note that this classification is currently binary. We are aware that some artists do not perform as male or female (e.g. Conchita Wurst) therefore this is not a completely accurate representation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_property(session, concept_id, property_id):\n",
    "    \"\"\"Async reimplementation of wikipeople.get_property\n",
    "    https://github.com/samvanstroud/wikipeople/blob/master/wikipeople/wikipeople.py\n",
    "    \n",
    "    session is an aiohttp ClientSession.\n",
    "    concept_id can be obtained using the get_concept_id function\n",
    "    property_id is hardcoded, I don't know where to get these from, but whatever.\n",
    "    \n",
    "    Returns None if any of this can't be found for whatever reason.\n",
    "    \n",
    "    e.g. \"Q219655\" is the concept_id for Carey Mulligan; \"P21\" is the property_id for gender. So we have that\n",
    "        get_property(session, \"Q219655\", \"P21\") -> \"female\"\n",
    "    \"\"\"\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action': 'wbgetclaims',\n",
    "              'entity': concept_id,\n",
    "              'property': property_id,\n",
    "              'language': 'en',\n",
    "              'format': 'json'}\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        try:\n",
    "            res = await resp.json()\n",
    "        except Exception as e:\n",
    "            print(resp)\n",
    "            raise e\n",
    "\n",
    "    if property_id not in res['claims']:\n",
    "        return None\n",
    "    # This gives yet another 'id', and we then need to perform yet another HTTP\n",
    "    # request to find the actual *label* that this corresponds to.\n",
    "    else:\n",
    "        id = None\n",
    "        for prop in res['claims'][property_id]:\n",
    "            try:\n",
    "                id = prop['mainsnak']['datavalue']['value']['id']\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if id is None:\n",
    "            return None\n",
    "        else:\n",
    "            new_params =  {'action': 'wbgetentities',\n",
    "                           'ids': id,\n",
    "                           'languages': 'en',\n",
    "                           'format': 'json',\n",
    "                           'props': 'labels'}\n",
    "            async with session.get(url, params=new_params) as resp:\n",
    "                try:\n",
    "                    res = await resp.json()\n",
    "                except Exception as e:\n",
    "                    print(resp)\n",
    "                    raise e\n",
    "            try:\n",
    "                return res['entities'][id]['labels']['en']['value']\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "async def get_concept_id(session, page_name):\n",
    "    \"\"\"\n",
    "    Get the concept_id corresponding to a particular Wikipedia page. For some odd reason, some Wikipedia\n",
    "    pages don't have concept IDs. In such a case, we return None.\n",
    "    \n",
    "    e.g. get_concept_id(session, \"Carey Mulligan\") -> \"Q219655\"\n",
    "    \"\"\"\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action': 'wbsearchentities',\n",
    "              'search': page_name,\n",
    "              'language': 'en',\n",
    "              'format': 'json'}\n",
    "    music_markers = [\n",
    "        'singer', 'artist', 'musician', 'music',\n",
    "        'band', 'group', 'duo', 'ensemble'\n",
    "    ]\n",
    "\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        # Titles of WP pages that match the search query.\n",
    "        json = await resp.json()\n",
    "\n",
    "    result = json['search']\n",
    "\n",
    "    if len(result) == 0:\n",
    "        # Couldn't find a concept id for the person/group\n",
    "        return None\n",
    "\n",
    "    # By default, choose the first result from the list\n",
    "    target = 0\n",
    "    # But check the other results to see if any of them are musicians (as\n",
    "    # indicated by the markers) and Eurovision contestants\n",
    "    for i, res in enumerate(result):\n",
    "        if 'description' in res['display']:\n",
    "            description = res['display']['description']['value']\n",
    "            if any(markers in description for markers in music_markers):\n",
    "                concept_id = res['id']\n",
    "                contestant_in = await get_property(session, concept_id, 'P1344')\n",
    "                if contestant_in is not None and \"Eurovision\" in contestant_in:\n",
    "                    target = i\n",
    "    # Return the concept ID of the result found\n",
    "    return result[target]['id']\n",
    "\n",
    "async def lookup_gender(session, page_name):\n",
    "    \"\"\"Find gender of a performing act, using the name associated with their\n",
    "    Wikipedia page. Returns None if could not be found.\n",
    "    \"\"\"\n",
    "    concept_id = await get_concept_id(session, page_name)\n",
    "    if concept_id is None:\n",
    "        return None\n",
    "\n",
    "    gender = await get_property(session, concept_id, 'P21')\n",
    "    instance = await get_property(session, concept_id, 'P31')\n",
    "    if gender is None and instance is None:\n",
    "        return None\n",
    "    elif gender is None and instance is not None:\n",
    "        group_checks = [\"group\", \"duo\", \"trio\", \"music\", \"band\", \"ensemble\"]\n",
    "        if any(x in instance for x in group_checks):\n",
    "            return \"group\"\n",
    "    else:\n",
    "        return gender\n",
    "\n",
    "async def get_pages(session, name):\n",
    "    \"\"\"Obtain a list of Wikipedia pages obtained by searching for a name.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"opensearch\",\n",
    "        \"namespace\": \"0\",\n",
    "        \"search\": name,\n",
    "        \"limit\": \"10000\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    async with session.get(url, params=params) as resp:\n",
    "        # Titles of WP pages that match the search query.\n",
    "        json = await resp.json()\n",
    "    return json[1]\n",
    "\n",
    "async def get_artist_gender(session, name):\n",
    "    gender = None\n",
    "    # Get the WP page for this person/group\n",
    "    pages = await get_pages(session, name)\n",
    "    # If there's one, try to get their gender from the first page\n",
    "    if len(pages) > 0:\n",
    "        gender = await lookup_gender(session, pages[0])\n",
    "    # Finally we use some heuristics to cover some edge cases\n",
    "    if gender is None:\n",
    "        if '&' in name or 'feat.' in name:\n",
    "            return 'group'\n",
    "    \n",
    "    return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the gender data has already been saved. If so, load it in.\n",
    "p = Path(\"data/gender_dict.json\")\n",
    "if p.is_file():\n",
    "    with open(p, 'r') as file:\n",
    "        gender_dict = json.load(file)\n",
    "        print('Loaded performer genders from file')\n",
    "        \n",
    "else:\n",
    "    # If not, now that we have all the necessary functionality, we can fetch the data from Wikipedia.\n",
    "    all_performers = df_VL['Artist'].unique().tolist()\n",
    "    n_performers = len(all_performers)\n",
    "    MAX_CONCURRENT = 40   # To stop Wikipedia from complaining about 'too many requests'\n",
    "    USER_AGENT = 'Eurovision study @ The Alan Turing Institute mailto:jyong@turing.ac.uk'\n",
    "\n",
    "    async def get_all_genders():\n",
    "        genders = []\n",
    "        print(f'We need to fetch the genders of {n_performers} performers, in batches of {MAX_CONCURRENT}. Hold tight...')\n",
    "        async with aiohttp.ClientSession(headers={'User-Agent': USER_AGENT}) as session:\n",
    "            start = 0\n",
    "            end = MAX_CONCURRENT\n",
    "            while start < n_performers:\n",
    "                print(f'Getting genders for performers #{start + 1} to #{end}... ', end='')\n",
    "                batch_tasks = asyncio.gather(*[get_artist_gender(session, p) for p in all_performers[start:end]])\n",
    "                batch_genders = await batch_tasks\n",
    "                print(f'Got {len(batch_genders)} results, {len([g for g in batch_genders if g is None])} of which were None.')\n",
    "                genders = genders + batch_genders\n",
    "                start = end\n",
    "                end = min(end + MAX_CONCURRENT, n_performers)\n",
    "                await asyncio.sleep(1.5)   # Put a pause between batches to avoid being timed out\n",
    "        # now pray that I didn't make an off-by-one error somewhere\n",
    "        assert len(genders) == n_performers\n",
    "        print('Finished downloading gender data.')\n",
    "        return dict(zip(all_performers, genders))\n",
    "        \n",
    "    gender_dict = await get_all_genders()\n",
    "    \n",
    "    # Manually assign missing entries (the Nones).\n",
    "    male = ['Michael Hajiyanni', 'Charlie', 'Tüzmen', 'Mietek Szcześniak', 'Olexandr', 'Max', 'Brinck',\n",
    "            'Sakis Rouvas (2)', 'Gianluca', 'Frans', 'Chingiz', 'Mahmood', 'Serhat (2)', 'Miki', 'Stefan']\n",
    "    female = ['Gunvor', 'Selma', 'Charlotte Nilsson (Perrelli)', 'Karolina', 'Laura', 'Rosa', 'Lou', 'Nicola',\n",
    "            'Karmen', 'Sanda', 'Ortal', 'Gracia', 'Chiara (2)', 'Hanna', 'Chiara (3)', 'Elena', 'Lena (2)',\n",
    "            'Birgit', 'Samra', 'ZAA Sanja Vučić', 'Anja', 'Alma', 'Netta', 'Michela', 'Efendi', 'Victoria',\n",
    "            'Destiny', 'Amanda Georgiadi Tenfjord', 'MARO']\n",
    "    group = ['Eden', 'Voice', 'Taxi', 'One', 'Prime Minister', 'Fame', 'Regina (band)', 'ESDM',\n",
    "            'Tolmachevy Sisters', 'Minus One', 'AWS']\n",
    "    for p in male:\n",
    "        gender_dict[p] = \"male\"\n",
    "    for p in female:\n",
    "        gender_dict[p] = \"female\"\n",
    "    for p in group:\n",
    "        gender_dict[p] = \"group\"\n",
    "\n",
    "    # Wikipedia needs to learn that 'trans woman' is 'female'.\n",
    "    for k, v in gender_dict.items():\n",
    "        if v == 'trans woman':\n",
    "            gender_dict[k] = 'female'\n",
    "            \n",
    "    # Save it to a file\n",
    "    with open('data/gender_dict.json', 'w') as file:\n",
    "        json.dump(gender_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gender to the dataframe.\n",
    "df_VLG = df_VL.copy()\n",
    "df_VLG['gender'] = df_VLG['Artist'].map(gender_dict)\n",
    "df_VLG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_consistency(df_VLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_VLG.to_csv('data/gender.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration\n",
    "\n",
    "- The `migration-flows.csv` data is from [Our World in Data](https://ourworldindata.org/migration) on international migration, under the 'Explore data on where people migrate from and to' section.\n",
    "- Original source is from the UN.\n",
    "- Data shows total number of immigrants in each country split by country of origin in the years 1990-2020, recorded at intervals of every 5 years.\n",
    "- Additional population size data (`pop_sizes.csv`) is taken from the [World Bank](https://data.worldbank.org/indicator/SP.POP.TOTL?end=2021&start=2021&view=map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration = pd.read_csv('data/migration-flows.csv')\n",
    "\n",
    "# Martin actually writes good pandas code, unlike me\n",
    "\n",
    "migration = (migration\n",
    "    .pipe(pd.melt, id_vars=['Country', 'Year'], var_name='Migration', value_name='Count')  # to long format\n",
    "    .loc[lambda x: x['Migration'].str.contains('Emigrants')]                               # filter for emigrant rows\n",
    "    .pipe(lambda x: x.rename(columns = {col: col.lower() for col in x.columns}))           # lowercase column names                                                         \n",
    "    .assign(migration = lambda x: x.migration.str.replace('Emigrants from ', ''))          # filter for emigrant rows                          \n",
    "    .rename(columns={'migration': 'emigrated_from', 'country': 'emigrated_to'})            # boil down to country name\n",
    "    .query('count >= 0')                                                                   # negative counts are just total emigrants from country\n",
    "    .pipe(lambda x: x.assign(count = x['count'].astype(int)))                              # convert count to int     \n",
    ")\n",
    "migration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up country names\n",
    "for ft in ['from', 'to']:\n",
    "    migration[f'emigrated_{ft}'] = migration[f'emigrated_{ft}'].str.lower()\n",
    "    migration.loc[migration[f'emigrated_{ft}'] == 'moldova', f'emigrated_{ft}'] = 'moldova, republic of'\n",
    "    migration.loc[migration[f'emigrated_{ft}'] == 'russia', f'emigrated_{ft}'] = 'russian federation'\n",
    "\n",
    "# Remove countries we don't care about\n",
    "ev_countries = set(df_VLG['from_country'].unique()).union(set(df_VLG['to_country'].unique()))\n",
    "migration = migration[(migration['emigrated_to'].isin(ev_countries)) & (migration['emigrated_from'].isin(ev_countries))]\n",
    "\n",
    "migration_countries = set(migration['emigrated_to'].unique()).union(set(migration['emigrated_from'].unique()))\n",
    "print(ev_countries - migration_countries)  # No data for Yugoslavia.\n",
    "\n",
    "# Add in country codes\n",
    "for ft in ['from', 'to']:\n",
    "    migration[f'emigrated_{ft}_code2'], migration[f'emigrated_{ft}_code3'] = zip(*migration[f'emigrated_{ft}'].map(get_country_codes))\n",
    "    \n",
    "migration = migration.reset_index(drop=True)\n",
    "migration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = (pd.read_csv('data/pop_sizes.csv')\n",
    "           .iloc[:, 3:]\n",
    "           .rename(columns=lambda x: x.lower().replace(' ', '_'))\n",
    "           .pipe(pd.melt, id_vars=['country_code'], var_name='year', value_name='population')\n",
    "           .assign(year=lambda x: x['year'].apply(lambda y: y.split('_')[0]))\n",
    "           .assign(year=lambda x: x['year'].astype(int))\n",
    "           .rename(columns={'country_code': 'code3'})\n",
    "           .dropna()\n",
    "           .assign(population=lambda x: pd.to_numeric(x['population'], errors='coerce'))\n",
    ")\n",
    "pop_size.head()\n",
    "\n",
    "migration_and_pop = (migration.merge(pop_size, left_on=['year', 'emigrated_to_code3'], right_on=['year', 'code3'], how='left')\n",
    "                     .rename(columns={'population': 'population_to'})\n",
    "                    .assign(prop_emigrants=lambda x: x['count'] / x['population_to'])\n",
    "                    #.reindex(columns=['country', 'code', 'code3', 'population', 'year', 'emigrated_from_code', 'count', 'prop_emigrants'])\n",
    "                    )\n",
    "migration_and_pop.head(n=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't have migration data for every year, when merging with the main dataset, we take the last migration data point before the competition.\n",
    "So, for example, the 2012 entries will contain migration data from 2010.\n",
    "\n",
    "To do this, we'll first make 5 copies of each row from the `migration_and_pop` dataframe, each with a different year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_and_pop['migration_pop_year'] = migration_and_pop['year']\n",
    "\n",
    "total_migration_and_pop = migration_and_pop.copy()\n",
    "for i in range(1, 5):\n",
    "    next_migration_and_pop = migration_and_pop.copy()\n",
    "    next_migration_and_pop['year'] = next_migration_and_pop['year'] + i\n",
    "    total_migration_and_pop = pd.concat([total_migration_and_pop, next_migration_and_pop], ignore_index=True)\n",
    "    \n",
    "total_migration_and_pop = total_migration_and_pop.sort_values(by=[\"emigrated_from\", \"emigrated_to\", \"year\"])\n",
    "\n",
    "total_migration_and_pop = total_migration_and_pop[['emigrated_from_code2', 'emigrated_to_code2', 'year', 'count', 'population_to', 'prop_emigrants', 'migration_pop_year']]\n",
    "\n",
    "total_migration_and_pop.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can join with the main dataframe.\n",
    "\n",
    "# migration_v2p      -> number of migrants from voting country to performing country\n",
    "# population_p       -> population of performing country\n",
    "# prop_emigrants_v2p -> proportion of migrants from voting country in population of performing country\n",
    "df_VLGM = df_VLG.merge(total_migration_and_pop, how='left', left_on=['from_code2', 'to_code2', 'year'], right_on=['emigrated_from_code2', 'emigrated_to_code2', 'year'])\n",
    "df_VLGM = (df_VLGM\n",
    "           .drop(columns=['emigrated_from_code2', 'emigrated_to_code2', 'migration_pop_year'])\n",
    "           .rename(columns={'count': 'migration_v2p', 'population_to': 'population_p', 'prop_emigrants': 'prop_emigrants_v2p'})\n",
    ")\n",
    "\n",
    "# migration_p2v      -> number of migrants from performing country to voting country\n",
    "# population_p       -> population of voting country\n",
    "# prop_emigrants_v2p -> proportion of migrants from performing country in population of voting country\n",
    "# migration_pop_year -> year from which the migration and population data is taken\n",
    "df_VLGM = df_VLGM.merge(total_migration_and_pop, how='left', left_on=['from_code2', 'to_code2', 'year'], right_on=['emigrated_to_code2', 'emigrated_from_code2', 'year'])\n",
    "df_VLGM = (df_VLGM\n",
    "           .drop(columns=['emigrated_from_code2', 'emigrated_to_code2'])\n",
    "           .rename(columns={'count': 'migration_p2v', 'population_to': 'population_v', 'prop_emigrants': 'prop_emigrants_p2v'})\n",
    ")\n",
    "df_VLGM['migration_pop_year'] = df_VLGM['migration_pop_year'].astype(int, errors='ignore')   # ignore NaN's.\n",
    "\n",
    "df_VLGM.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_VLGM.to_csv('data/migration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comps without win\n",
    "\n",
    "Copy the winners from wikipedia. Note that Luxembourg withdrew from the contest in 1994, so is not included in our data - hence why it does not merge, we will remove this from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    f\"https://en.wikipedia.org/wiki/List_of_Eurovision_Song_Contest_winners\"\n",
    ")\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "table = soup.find_all(\"table\", {\"class\": \"wikitable\"})[0]\n",
    "winners = pd.DataFrame(pd.read_html(str(table))[0])\n",
    "\n",
    "winners = winners.loc[winners['Year'] != 2020, ['Year', 'Country']]\n",
    "winners = list(winners.to_records(index=False))\n",
    "\n",
    "winners = [(year, 'Russian federation' if Country == 'Russia' else Country) for year, Country in winners]\n",
    "winners = [(year, get_country_codes(Country.lower())[0]) for year, Country in winners if Country != 'Luxembourg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary mapping each country to the years they won in.\n",
    "all_wins = {}\n",
    "\n",
    "# for each row in winners, get the country code and year\n",
    "for y, code in winners:\n",
    "    if code in all_wins:\n",
    "        all_wins[code].append(y)\n",
    "    else:\n",
    "        all_wins[code] = [y]\n",
    "\n",
    "print(all_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VLGMC = df_VLGM.copy()\n",
    "\n",
    "def comps_without_win(code, year):\n",
    "      # Find last win. Use 1955 (year before ESC started) if there isn't one.\n",
    "      if code not in all_wins:\n",
    "        last_win = 1955\n",
    "      else:\n",
    "        last_win = max([y for y in all_wins[code] if y < year], default=1955)\n",
    "      \n",
    "      # Count the number of competitions since the last win. Note that the 2020\n",
    "      # contest was cancelled.\n",
    "      comps = year - last_win - 1\n",
    "      if year > 2020 and last_win < 2020:\n",
    "        comps = comps - 1\n",
    "\n",
    "      return comps\n",
    "\n",
    "# Some quick tests.\n",
    "assert(comps_without_win(\"UA\", 2023) == 0)   # won in 2022\n",
    "assert(comps_without_win(\"GB\", 2023) == 24)  # won in 1997\n",
    "assert(comps_without_win(\"AU\", 2023) == 66)  # never won\n",
    "assert(comps_without_win(\"SE\", 1983) == 8)   # won in 1974\n",
    "assert(comps_without_win(\"SE\", 2019) == 3)   # won in 2015\n",
    "assert(comps_without_win(\"NL\", 2019) == 43)  # won in 1975\n",
    "\n",
    "df_VLGMC['comps_without_win'] = df_VLGMC.apply(lambda row: comps_without_win(row['to_code2'], row['year']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VLGMC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_VLGMC.to_csv('data/comps_without_win.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Border data\n",
    "\n",
    "Raw data is obtained from GeoDataSource: https://github.com/geodatasource/country-borders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border = pd.read_csv('data/geodatasource-country-borders.csv')\n",
    "border.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data first; subset to only Eurovision countries\n",
    "ev_code2s = set(df_VLGMC['from_code2'].unique()).union(set(df_VLGMC['to_code2'].unique()))\n",
    "border = border[(border['country_code'].isin(ev_code2s)) & (border['country_border_code'].isin(ev_code2s))]\n",
    "\n",
    "# Generate a list of tuples\n",
    "border_tuples = list(border[['country_code', 'country_border_code']].itertuples(index=False, name=None))\n",
    "# Sanity check to make sure the list is symmetric. Expect True.\n",
    "all((b, a) in border_tuples for a, b in border_tuples)\n",
    "\n",
    "# Yugoslavia needs a manual exception. For now, Yugoslavia shares a border with \n",
    "# country X if X shares a border with either Serbia or Montenegro.\n",
    "def has_border(cty1, cty2):\n",
    "    if cty1 == 'YU':\n",
    "        return has_border('RS', cty2) or has_border('ME', cty2)\n",
    "    elif cty2 == 'YU':\n",
    "        return has_border(cty1, 'RS') or has_border(cty1, 'ME')\n",
    "    else:\n",
    "        return (cty1, cty2) in border_tuples\n",
    "# TODO: CHECK IF THIS IS HISTORICALLY CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_border('BA', 'YU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then just add a new column to df_VLGMC that is True if the two countries are neighbours.\n",
    "\n",
    "df_VLGMCB = df_VLGMC.copy()\n",
    "df_VLGMCB[\"has_border\"] = df_VLGMCB.apply(lambda row: has_border(row['from_code2'], row['to_code2']), axis=1)\n",
    "df_VLGMCB.head(n=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a row to represent the competition round the votes apply to (which for historical data, is always the final)\n",
    "df_VLGMCB['comp_round'] = 'f'\n",
    "\n",
    "df_VLGMCB.to_csv('data/df_main.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the 2023 Performance data \n",
    "\n",
    "\n",
    "This will be used for the purposes of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderGuess():\n",
    "    \"\"\"Guess the gender of a performer based on entries from Wikidata.\n",
    "    \"\"\"\n",
    "    future = False\n",
    "    exceptions = {\"Brunette\": \"female\"}\n",
    "\n",
    "    def __init__(self, future=False):\n",
    "        print(\"Initialising gender guesser\")\n",
    "        pass\n",
    "\n",
    "    def __search_wikidata(self, string):\n",
    "        \"\"\"\n",
    "        Query the Wikidata API using the wbsearchentities function.\n",
    "        Return the concept ID of the search result that has the musician identifier.\n",
    "        \"\"\"\n",
    "        query = 'https://www.wikidata.org/w/api.php?action=wbsearchentities&search='\n",
    "        query += string\n",
    "        query += '&language=en&format=json'\n",
    "        music_markers = [\n",
    "            'singer', 'artist', 'musician', 'music',\n",
    "            'band', 'group', 'duo', 'ensemble'\n",
    "        ]\n",
    "        res = requests.get(query).json()\n",
    "        if len(res['search']) == 0:\n",
    "            return None\n",
    "\n",
    "        target = 0\n",
    "        for i in range(len(res['search'])):\n",
    "            if 'description' in res['search'][i]['display']:\n",
    "                description = res['search'][i]['display']['description']['value']\n",
    "                if any(markers in description.lower() for markers in music_markers):\n",
    "                    if self.future:\n",
    "                        target = i\n",
    "                        break\n",
    "                    else:\n",
    "                        concept_id = res['search'][i]['id']\n",
    "                        contestant_in = wp.get_property(concept_id, 'P1344')[-1]\n",
    "                        if \"Eurovision\" in contestant_in:\n",
    "                            target = i\n",
    "                            break\n",
    "\n",
    "        return res['search'][target]['id']\n",
    "\n",
    "    def __lookup_gender(self, name):\n",
    "        \"\"\"Find gender of given name. If the name is not related to a wiki entry \n",
    "        it will return 'RNF' (record not found). Alternatively it will return \n",
    "        the gender if the record has one or NA if it does not have this \n",
    "        property.\n",
    "        Args:\n",
    "            name (str): The name to search\n",
    "        Returns:\n",
    "            str: The gender of the person searched\n",
    "        \"\"\"\n",
    "        gender = 'RNF'\n",
    "        data = self.__search_wikidata(name)\n",
    "        if data:\n",
    "            gender = wp.get_property(data, 'P21')[-1]\n",
    "            instance = wp.get_property(data, 'P31')[-1]\n",
    "            if gender == 'NA':\n",
    "                group_checks = [\n",
    "                    \"group\", \"duo\", \"trio\", \"music\", \"band\", \"ensemble\"\n",
    "                ]\n",
    "                if any(x in instance for x in group_checks):\n",
    "                    gender = \"group\"\n",
    "        return gender\n",
    "\n",
    "    def __get_artist_gender(self, search):\n",
    "        if search in self.exceptions:\n",
    "            return self.exceptions[search]\n",
    "\n",
    "        s = requests.Session()\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"opensearch\",\n",
    "            \"namespace\": \"0\",\n",
    "            \"search\": search,\n",
    "            \"limit\": \"10000\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = s.get(url=url, params=params)\n",
    "        names = r.json()[1]\n",
    "        gender = \"\"\n",
    "        if len(names) < 1:\n",
    "            gender = \"RNF\"\n",
    "        else:\n",
    "            for n in names:\n",
    "                gender = self.__lookup_gender(n)\n",
    "                if not any(gender == x for x in [\"RNF\", \"NA\"]):\n",
    "                    return gender\n",
    "        if gender == \"RNF\" or gender == \"NA\":\n",
    "            if (\"&\" in search) or (' and ' in search):\n",
    "                gender = \"group\"\n",
    "        return gender\n",
    "\n",
    "    def guess_gender(self, artist):\n",
    "        \"\"\"Guess the gender of an artist.\n",
    "        Returns a string representing the gender of the artist.\n",
    "        Args:\n",
    "            artist (str): The name of the artist\n",
    "        Returns:\n",
    "            str: One of \"group\", \"female\", \"male\" or \"RNF\"\n",
    "        \"\"\"\n",
    "        gender = self.__get_artist_gender(artist)\n",
    "        #print(\"Artist: {}, gender: {}\".format(artist, gender))\n",
    "        return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VLGMCB['from_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from wikipeople import wikipeople as wp # use wp to get the gender data as needed.\n",
    "\n",
    "class EurovisionFuture():\n",
    "    \"\"\"Collect together information about acts that might appear in the\n",
    "    Eurovision final, based on their appearance in the semi finals.\n",
    "    \"\"\"\n",
    "    in_file = None\n",
    "    out_file = None\n",
    "    countries = None\n",
    "    result = None\n",
    "    language_map = None\n",
    "    df = None\n",
    "    language_match = None\n",
    "    gender = None\n",
    "\n",
    "    def __init__(self, in_file, out_file):\n",
    "        self.in_file = in_file\n",
    "        self.out_file = out_file\n",
    "\n",
    "        # Don't run if the files aren't present\n",
    "        for file in [in_file,]:\n",
    "            if not file.is_file():\n",
    "                raise FileNotFoundError(f\"file {str(file)} was not found\")\n",
    "\n",
    "        #self.__load_country_codes()\n",
    "        self.__import_existing()\n",
    "        #self.language_match = LanguageMatch(self.country_codes_dict)\n",
    "        self.gender = GenderGuess(future=True)\n",
    "\n",
    "    def __expand_data(self, collected):\n",
    "        \"\"\"Expand the data using a cross join on voting countries.\n",
    "        For each row in the data frame, generate a duplicate for each country\n",
    "        that might vote for it, filling the \"From country\" column with the\n",
    "        voting country.\n",
    "        Args:\n",
    "            collected (pd.DataFrame): the current data to be expanded\n",
    "        Returns:\n",
    "            pd.DataFrame: the existing data expanded with the \"From country\" column\n",
    "        \"\"\"\n",
    "        from_country = pd.DataFrame({\"from_country\": df_VLGMCB['from_country'].unique()})\n",
    "        collected = pd.merge(collected, from_country, how=\"cross\")\n",
    "\n",
    "        collected['from_code2'], collected['from_code3'] = zip(*collected['from_country'].map(get_country_codes))\n",
    "\n",
    "        # Remove rows where country and from_country are the same (self-voting)\n",
    "        collected = collected.drop(collected[collected[\"to_code2\"] == collected[\"from_code2\"]].index)\n",
    "\n",
    "        # Reset the index\n",
    "        collected = collected.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        return collected\n",
    "\n",
    "    def __transfer_existing_data(self, collected):\n",
    "        \"\"\"Transfer data from the historical data frame into the future frame.\n",
    "        Transfers border and migration data from the most recent year for which\n",
    "        it exists in the historical data into the future data frame.\n",
    "        Args:\n",
    "            collected (pd.DataFrame): the current data to be augmented\n",
    "        Returns:\n",
    "            pd.DataFrame: data updated with border and migration details\n",
    "        \"\"\"\n",
    "        # Fill out the has_border entries from existing data\n",
    "        print('Transfering border data')\n",
    "        collected['has_border'] = collected[['to_code2', 'from_code2']].apply(lambda x: self.__get_has_border(x['to_code2'], x['from_code2']), axis=1)\n",
    "\n",
    "        # Fill the voting language data\n",
    "        collected = pd.merge(collected, df_voting_language[['Country_code2', 'Voting_Languages']], left_on='from_code2', right_on='Country_code2', how='left')\n",
    "        collected['Contains_Voting_Language'] = collected.apply(lambda df: len(set(df['Language_sung']).intersection(df['Voting_Languages'].split())) > 0, axis=1)\n",
    "\n",
    "        # Fill out migration data\n",
    "        print('Transfering migration data')\n",
    "        migration_pop_cols = ['migration_v2p', 'prop_emigrants_v2p', 'migration_p2v',\n",
    "       'population_v', 'prop_emigrants_p2v', 'migration_pop_year']\n",
    "\n",
    "        collected[migration_pop_cols] = collected[['to_code2', 'from_code2']].apply(lambda x: self.__get_migration(x['to_code2'], x['from_code2']), axis=1)\n",
    "        \n",
    "        collected['points'] = np.nan\n",
    "        collected['total_points'] = np.nan\n",
    "        collected['rank'] = np.nan\n",
    "\n",
    "        print(collected['to_code2'].unique())\n",
    "\n",
    "        return collected\n",
    "\n",
    "    def __get_has_border(self, code, from_country):\n",
    "        \"\"\"Check whether there's a land border bertween two countries\n",
    "        Args:\n",
    "            code (str): the country code of one country\n",
    "            from_country (str): the country code of another country\n",
    "        Returns:\n",
    "            iot: True if the countries share a land border, False otherwise\n",
    "        \"\"\"\n",
    "        result = self.df[(self.df['to_code2'] == code) & (self.df['from_code2'] == from_country)].nlargest(1, 'year')['has_border'].values\n",
    "        return result[0] if len(result) > 0 else False\n",
    "\n",
    "    def __get_migration(self, code, from_country):\n",
    "        \"\"\"Get migration data from one countrhy to another\n",
    "        Args:\n",
    "            code (str): the country code for migrating to\n",
    "            from_country (str): the country code for migraition from\n",
    "        Returns:\n",
    "            [band, year, count, prop]: a list containing respectively the migration band (year), the year the data relates to\n",
    "                                       the number of migratnts and the proportion of population of the country migrating from\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        migration_pop_cols = ['migration_v2p', 'prop_emigrants_v2p', 'migration_p2v',\n",
    "       'population_v', 'prop_emigrants_p2v', 'migration_pop_year']\n",
    "\n",
    "        result = self.df[(self.df['to_code2'] == code) & (self.df['from_code2'] == from_country)].nlargest(1, 'year')[migration_pop_cols].values\n",
    "        result = result[0] if len(result) > 0 else [0] * len(migration_pop_cols)\n",
    "        return pd.Series(result)#, index=[migration_pop_cols])\n",
    "\n",
    "    def __get_population(self, code):\n",
    "        \"\"\"Get the most recent population info for a country\n",
    "        Args:\n",
    "            code (str): the country code for the country to check\n",
    "        Returns:\n",
    "            int: population data for the country for the most recent year found in the historical data frame\n",
    "        \"\"\"\n",
    "        result = self.df[self.df['to_code2'] == code][['year', 'population_p']].nlargest(1, 'year')['population_p'].values\n",
    "        return result[0] if len(result) > 0 else 0\n",
    "    \n",
    "    def process(self, explode):\n",
    "        \"\"\"Process the data\n",
    "        Downloads and processes the data for the semi finals in order to \n",
    "        generate data for the finals.\n",
    "        \n",
    "        If the explode parameter is set to True, the output will contain multiple rows for each\n",
    "        country, one for each voting country. If set to False, each country will have only a\n",
    "        single row.\n",
    "        \n",
    "        Args:\n",
    "            explode (bool): set to True to multiply the rows up by voting countries, False otherwise\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the performance data\n",
    "        semi_final_1 = self.__import_participants_from_wiki(2023, 'Semi-final_1', 'sf1')\n",
    "        semi_final_2 = self.__import_participants_from_wiki(2023, 'Semi-final_2', 'sf2')\n",
    "        semi_final_3 = self.__import_participants_from_wiki(2023, 'Final', 'f')\n",
    "        semi_finals = pd.concat([semi_final_1, semi_final_2, semi_final_3])\n",
    "\n",
    "        # Populate the vote-performance data\n",
    "        if explode:\n",
    "            semi_finals = self.__expand_data(semi_finals)\n",
    "            semi_finals = self.__transfer_existing_data(semi_finals)\n",
    "        else:\n",
    "            # Clear the columns that don't make sense without a \"from_country\" voter\n",
    "            for column in ['from_country', 'points', 'total_points', 'rank', 'from_code2', 'from_code3', \n",
    "                           'migration_v2p', 'prop_emigrants_v2p', 'migration_p2v', 'has_border',\n",
    "                           'migration_pop_year', 'prop_emigrants_p2v', 'population_v']:\n",
    "                semi_finals[column] = np.nan\n",
    "\n",
    "        # if any df_VLGMCB columns are not in semi-finals, raise an exception\n",
    "        if len(set(df_VLGMCB.columns) - set(semi_finals.columns)) > 0:\n",
    "            raise Exception(\"Missing columns in semi_finals:\", set(df_VLGMCB.columns) - set(semi_finals.columns))\n",
    "\n",
    "        # Match columns\n",
    "        semi_finals = semi_finals[df_VLGMCB.columns]\n",
    "\n",
    "        # Store the result\n",
    "        self.result = semi_finals\n",
    "\n",
    "    def get_result(self):\n",
    "        \"\"\"Get the result of all that processing\n",
    "        As long as processing has been performed, this will return a data frame of data for the\n",
    "        acts that are likely to appear in the Eurovision final based on their appearance in the\n",
    "        semi finals.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            pd.DataFrame: the processed data\n",
    "        \"\"\"\n",
    "        return self.result\n",
    "\n",
    "    def __import_existing(self):\n",
    "        \"\"\"\n",
    "        Import the 1998-2022 data set. We need it later do copy some data into the 2023 rows.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Read in CSV\n",
    "        self.df = pd.read_csv(self.in_file)\n",
    "        self.df['has_border'] = self.df['has_border'].fillna(0)\n",
    "\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "    def __last_win(self, country, year):\n",
    "        \"\"\"Returns the number of years since the last win\n",
    "        Returns the number of years since the last win based on the historical data.\n",
    "        This does not get updated for the most recent winner.\n",
    "        Args:\n",
    "            country (str): the country code for the country to get the info for\n",
    "            year (int): the current year\n",
    "        Returns:\n",
    "            int: the number of years since the country last won\n",
    "        \"\"\"\n",
    "        result = self.df[self.df['to_code2'] == country][['year', 'comps_without_win']].nlargest(1, 'year').values\n",
    "        recent_year, duration = result[0] if len(result) > 0 else [0, 0] \n",
    "        # We're going to assume year > recent_year to make our lives easier\n",
    "        assert(year > recent_year)\n",
    "        return duration + (year - recent_year)\n",
    "\n",
    "    def __import_participants_from_wiki(self, year, header_id, comp_round):\n",
    "        \"\"\"Download the data from the wiki for a particular semi final\n",
    "        Downloads artist data from Wikipedia based on the year and the subheading anchor.\n",
    "        Data is scraped from the first table in the section with the anchor tag or id provided.\n",
    "        Args:\n",
    "            year (int): the year to get the data for\n",
    "            header_id (str): the anchor for the section to scrapte the table data from\n",
    "        \"\"\"\n",
    "        url=f\"https://en.wikipedia.org/wiki/Eurovision_Song_Contest_{year}\"\n",
    "        print('Downloading wikipedia page: {}'.format(url))\n",
    "        response=requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tables = soup.find_all('table',{'class':\"wikitable\"})\n",
    "\n",
    "        table = soup.find(id=header_id).find_all_next('table')[0]\n",
    "\n",
    "        df_table=pd.read_html(str(table))\n",
    "        df_table=pd.DataFrame(df_table[0])\n",
    "\n",
    "        # Remove citations references from the column titles\n",
    "        df_table.columns = [x.split('[')[0] for x in df_table.columns]\n",
    "\n",
    "        # These values are the same for every row\n",
    "        df_table['year'] = year\n",
    "\n",
    "        # Country codes\n",
    "        df_table['to_country'] = df_table['Country'].str.lower()\n",
    "        df_table['to_country'] = df_table['to_country'].map(standardise_country)\n",
    "        df_table['to_code2'], df_table['to_code3'] = zip(*df_table['to_country'].map(get_country_codes))\n",
    "\n",
    "        # Convert the entry to a list of languages and strip any citation references\n",
    "        print('Deriving language entries')\n",
    "        df_table['Language_sung'] = df_table['Language(s)'].apply(lambda x: [y.lower().split('[')[0] for y in x.split(', ')])\n",
    "        df_table['Contains_English'] = df_table['Language_sung'].apply(lambda x: 'english' in x)\n",
    "        df_table['Contains_NonEnglish'] = df_table['Language_sung'].apply(lambda x: x != ['english'])\n",
    "\n",
    "        # print any countries in songs['Country'] that are not in df_languages['Country/Region']\n",
    "        if len(set(songs['Country_code2']) - set(df_languages['Country_code2'])) > 0: \n",
    "            countries = list(set(songs['Country_code2']) - set(df_languages['Country_code2']))\n",
    "            raise KeyError(\"Country name \" + ', '.join(countries) + \" was in songs, but not in df_languages.\")\n",
    "\n",
    "        # merge df_languages and language on Country and Country/Region\n",
    "        df_table = pd.merge(df_table, df_languages[['Country_code2', 'Official_languages']], left_on='to_code2', right_on='Country_code2', how='left')\n",
    "        \n",
    "        # Tidy the languages column\n",
    "        df_table['Official_languages'] = df_table['Official_languages'].fillna(' ')\n",
    "        df_table['Contains_Multiple_Languages'] = df_table['Language_sung'].apply(lambda x: len(x) > 1)\n",
    "        df_table['Number_of_Languages'] = df_table['Language_sung'].apply(get_n_languages)\n",
    "        df_table['Contains_Own_Language'] = df_table.apply(lambda df: len(set(df['Language_sung']).intersection(df['Official_languages'].split())) > 0, axis=1)\n",
    "\n",
    "\n",
    "        # Figure out the gender from Wikipedia\n",
    "        print('Guessing genders')\n",
    "        df_table['gender'] = df_table['Artist'].apply(lambda x: self.gender.guess_gender(x))\n",
    "\n",
    "        # Copy over data from the existing dataset\n",
    "        print('Transfering population data')\n",
    "        df_table['population_p'] = df_table['to_code2'].apply(lambda x: self.__get_population(x))\n",
    "\n",
    "        print('Calculating last win')\n",
    "        df_table['comps_without_win'] = df_table['to_code2'].apply(lambda x: self.__last_win(x, year))\n",
    "        \n",
    "        # Add in the competition round data\n",
    "        df_table['comp_round'] = comp_round\n",
    "\n",
    "        # Add in the competition round data\n",
    "        df_table['comp_round'] = comp_round\n",
    "\n",
    "        return df_table\n",
    "\n",
    "    def save(self, out_file):\n",
    "        \"\"\"Save the result to a CSV file\n",
    "        \"\"\"\n",
    "        print(\"Writing out results to: {}\".format(out_file))\n",
    "        self.result.to_csv(out_file)\n",
    "\n",
    "def print_syntax():\n",
    "\tprint('Syntax: get_future_performers.py <input-file> <country-pickle> <out-file>')\n",
    "\tprint()\n",
    "\tprint('\\tCollect data about future Eurovision performers')\n",
    "\tprint('\\t<input-file>     : CSV file containing data for previous years')\n",
    "\tprint('\\t<country-pickle> : pickle file mapping countries to country codes')\n",
    "\tprint('\\t<out-file>       : file to save the output CSV to')\n",
    "\tprint()\n",
    "\tprint('Example usage')\n",
    "\tprint('\\tget_future_performers.py eurovision_merged_covariates_03Feb.csv country_codes_dict.pickle out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = Path(\"data/df_main.csv\")\n",
    "out_file = Path(\"data/eurovision_2023.csv\")\n",
    "\n",
    "print(\"Historical data CSV: {}\".format(in_file))\n",
    "print(\"Out file: {}\".format(out_file))\n",
    "\n",
    "# # Initalise\n",
    "future = EurovisionFuture(in_file, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell takes around 2 mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.process(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result['to_country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result.to_csv('data/df_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
